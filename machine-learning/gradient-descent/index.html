<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Gradient Descent</title>
  <meta name="description" content="Gradient Descent is at the core of machine learning, and I've seen it mentioned in every single machine learning publication I've read. In this post, we'll c...">

  <link href="https://fonts.googleapis.com/css?family=Lato:900|Work+Sans" rel="stylesheet">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://yaz.in/machine-learning/gradient-descent/">
  <link rel="alternate" type="application/rss+xml" title="Yazin Alirhayim" href="https://yazin.org/feed.xml">

  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

  <script>
  (function(){
  	/* permanently redirect all traffic to https, keeping the path */
    var fullPath, host, ptnHost, subPath, ptnSubPath, result;

    ptnHost = /^https?:\/\/([a-z0-9.]*)[:\d]*?\/.*$/i; // localhost, google.com
    ptnSubPath = /^http:\/\/(.*?)(\/.*)$/i; // *, where "http://host/*"

    fullPath = window.location.toString();
    host = ptnHost.exec(fullPath); host = host[host.length - 1];
    subPath = ptnSubPath.exec(fullPath);
    if(subPath !== null && host != "localhost" && host != "127.0.0.1") {
      /* production URL thats using HTTP -- redirect to HTTPS sub-path */
      window.location.protocol = "https:";
    }
  })(); 
  </script>
</head>

<body>
<header> 
  <div class="links"> 
  
    
  
     
      <a class="page-link" href="/bookshelf/">My Bookshelf</a>
    
  
    
  
     
      <a class="page-link" href="/experiments/">Experiments</a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
     
      <a class="page-link" href="/projects/">Projects</a>
    
  
    
  
    
  
    
  
    
  
  </div>
</header>


  
    <h1 class="page-title">Gradient Descent</h1>
  

  <div class="meta"><span>December 19, 2016</span></div>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>So now we have a hypothesis function and we’ve got a way to measure how accurate it is. All we need now is a way to improve our hypothesis function, eventually finding the best possible \(\theta_0, \theta_1\) values (that minimize our cost).</p>

<p>That’s where <strong>gradient descent</strong> comes in.</p>

<p>Imagine plotting our \(\theta_0, \theta_1\) parameters on the \(x, y\) axes .. and a 3 dimensional \(y\) axis showing the corresponding cost function \(J(\theta_0, \theta_1)\) for those \(\theta_1, \theta_1\) values.</p>

<p>Here’s what that would look like:</p>

<p><img src="/assets/machine-learning/cost-function-vs-thetas.png" alt="Cost function vs thetas" /></p>

<p>What we’re looking for is the point on the graph would the <strong>lowest cost</strong>, which would represent the pit in the center. That’s the sweet spot.. and the values of \(\theta_0\) and \(\theta_1\) at that point are what we are looking for (to optimize our holy hypothesis function \(h\)).</p>

<p>Since plotting these pretty graphs is not always possible (imagine what it would look like if you had just 20 features instead of 1).</p>

<p>Instead then, what we need to do is find a mathematical way to get the answers we need.</p>

<h3 id="the-concept">The Concept</h3>

<p>At the core of the gradient descent technique is a very simple idea. If we can take a starting point - any starting point - and then take a tiny step “downhill” (towards a point with a lower <em>cost</em>), then we’ll eventually get to a pit .. somewhere where we can’t descend any further without going back up. This point is known as the <strong>local minima</strong> and that’s what we’re aiming for.</p>

<p>The only question is: “how do we know which way is downhill?”.</p>

<p>Well, that’s where your old from high school - derivatives - come in. If you remember one thing about derivatives in school, it’s that a derivative is the slope at a given point (if you’re completely lost, check out <a href="https://www.khanacademy.org/math/differential-calculus/taking-derivatives/derivative-intro/v/calculus-derivatives-1">this excellent introduction to derivates</a>).</p>

<p>So all we need to do then is follow the derivative (the slope at a particular point), and it will give us a direction to move towards.</p>

<script type="math/tex; mode=display">\theta_j := \theta_j - \alpha[\text{derivative of J}]</script>

<script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)</script>

<h3 id="gradient-descent-for-linear-regression">Gradient Descent for Linear Regression</h3>

<p>Assuming we’ve only got one \(x\) and one \(y\) (carrying on the example from the last post), we can start substituting equations in to really drive this point home.</p>

<p>Let’s start by substituting for the value of \(J\) which we derived in <a href="/machine-learning/some-basic-definitions">the previous post</a> as:</p>

<script type="math/tex; mode=display">J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m \left( h(x^{(i)}) - y^{(i)} \right)^2</script>

<p>.. this results in:</p>

<script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} \left( \frac{1}{2m} \sum_{i=1}^m \left( h(x^{(i)}) - y^{(i)} \right)^2 \right)</script>

<p>Since we have two variables that we’re trying to optimize (namely \(\theta_0)\) and \(\theta_1\)), we have to use what’s known in the geeky world of mathematics as <em>partial derivatives</em>. While it may sound like I just turned all Math professor on you, it’s actually a simple concept.</p>

<p>It’s like a normal derivative, where you treat everything as a constant except the variable you are deriving by. If that’s not enough by way of introduction, check <a href="http://math.stackexchange.com/questions/70728/partial-derivative-in-gradient-descent-for-two-variables/189792#189792">this great explanation on the Math StackExchange</a> site.</p>

<p>Ok, so here are the final equations we’re looking for (in their simplified form, after rearranging and stuff):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}

\theta_0 & := \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m \left(\theta_0 + \theta_{1}x^{(i)} - y^{(i)}\right) \\

\theta_1 & := \theta_1 - \alpha \frac{1}{m} \sum_{i=1}^m \left(\theta_0 +
\theta_{1}x^{(i)} - y^{(i)}\right) x^{(i)}

\end{align} %]]></script>

<p>She’s a real beauty, ain’t she?</p>

<p>And this is just one step. We’ll be running this calculation many, many times .. as we journey down the hill of cost, to the local minima (sometimes thousands of times even).</p>

<h3 id="the-code">The code</h3>

<p>Nothing drives a point home like seeing some code, and I really struggled to get my head around this whole vector business until I actually saw it implemented in traditional arrays.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># y = theta_1 * x + theta_0
# Not actually required in the gradient descent calculation; just used to verify
# the sanity of the results :)
</span><span class="k">def</span> <span class="nf">compute_error_for_line_given_points</span><span class="p">(</span><span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">,</span> <span class="n">points</span><span class="p">):</span>
  <span class="n">totalError</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)):</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
      <span class="n">totalError</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="p">(</span><span class="n">theta_1</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">theta_0</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span>
  <span class="k">return</span> <span class="n">totalError</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">step_gradient</span><span class="p">(</span><span class="n">theta_0_current</span><span class="p">,</span> <span class="n">theta_1_current</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
  <span class="c1"># Gets called for each iteration of 'alpha'
</span>  <span class="n">theta_0_gradient</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">theta_1_gradient</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">m</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">theta_0_gradient</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="p">((</span><span class="n">theta_1_current</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">theta_0_current</span><span class="p">))</span>
    <span class="n">theta_1_gradient</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="p">((</span><span class="n">theta_1_current</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">theta_0_current</span><span class="p">))</span>
  <span class="n">new_theta_0</span> <span class="o">=</span> <span class="n">theta_0_current</span> <span class="o">-</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">theta_0_gradient</span><span class="p">)</span>
  <span class="n">new_theta_1</span> <span class="o">=</span> <span class="n">theta_1_current</span> <span class="o">-</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">theta_1_gradient</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">new_theta_0</span><span class="p">,</span> <span class="n">new_theta_1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">gradient_descent_runner</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">starting_theta_0</span><span class="p">,</span> <span class="n">starting_theta_1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">):</span>
  <span class="c1"># This method simply runs the 'step_gradient' method num_iterations times,
</span>  <span class="c1"># updating the values of theta_0, theta_1 after each iteration.
</span>  <span class="n">theta_0</span> <span class="o">=</span> <span class="n">starting_theta_0</span>
  <span class="n">theta_1</span> <span class="o">=</span> <span class="n">starting_theta_1</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
    <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span> <span class="o">=</span> <span class="n">step_gradient</span><span class="p">(</span><span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">,</span> <span class="n">array</span><span class="p">(</span><span class="n">points</span><span class="p">),</span> <span class="n">alpha</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">():</span>
  <span class="c1"># This method reads all of our data points (x, y)'s and calls the
</span>  <span class="c1"># 'gradient_descent_runner' passing in all of the variables
</span>  <span class="n">points</span> <span class="o">=</span> <span class="n">genfromtxt</span><span class="p">(</span><span class="s">"data.csv"</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">","</span><span class="p">)</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.0001</span>
  <span class="n">initial_theta_0</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># initial y-intercept guess
</span>  <span class="n">initial_theta_1</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># initial slope guess
</span>  <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">1000</span>
  <span class="k">print</span> <span class="s">"Starting gradient descent at theta_0 = {0}, theta_1 = {1}, error = {2}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">initial_theta_0</span><span class="p">,</span> <span class="n">initial_theta_1</span><span class="p">,</span> <span class="n">compute_error_for_line_given_points</span><span class="p">(</span><span class="n">initial_theta_0</span><span class="p">,</span> <span class="n">initial_theta_1</span><span class="p">,</span> <span class="n">points</span><span class="p">))</span>
  <span class="k">print</span> <span class="s">"Running..."</span>
  <span class="p">[</span><span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">]</span> <span class="o">=</span> <span class="n">gradient_descent_runner</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">initial_theta_0</span><span class="p">,</span> <span class="n">initial_theta_1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">)</span>
  <span class="k">print</span> <span class="s">"After {0} iterations theta_0 = {1}, theta_1 = {2}, error = {3}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">,</span> <span class="n">compute_error_for_line_given_points</span><span class="p">(</span><span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">,</span> <span class="n">points</span><span class="p">))</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
  <span class="n">run</span><span class="p">()</span></code></pre></figure>

<p>You’ll find the <code class="highlighter-rouge">data.csv</code> file <a href="https://gist.github.com/yazinsai/a962de1d2efcf3aa4986">here</a>.</p>

<p>Here’s what the output looks like:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">&gt;</span> <span class="nv">$python</span> gradient_descent_example.py
Starting gradient descent at theta_0 <span class="o">=</span> 0, theta_1 <span class="o">=</span> 0, error <span class="o">=</span> 2782.55391724
Running...
After 1000 iterations theta_0 <span class="o">=</span> 0.0590585566422, theta_1 <span class="o">=</span> 1.47833132745, error <span class="o">=</span> 56.3163353936</code></pre></figure>

<p>It took me a while to wrap my head around this, and to get the code to a working state. <a href="http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/">Matt Nedrich’s post</a> on the topic was very helpful.</p>

<p><em>Note: I’ve used Python in the example above, but going forward most examples will be written using <a href="http://www.wikiwand.com/en/GNU_Octave">Octave</a>.</em></p>




<footer> 
</footer>

<!-- Google Analytics Tracking code -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-89256258-1', 'auto');
  ga('send', 'pageview');

</script>



</body>
</html>
